# Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
This is the project page of the paper: Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
